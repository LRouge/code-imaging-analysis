{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561638d-3864-4a2d-adf1-34647303785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d75d0-a5be-4ed6-a4a9-ceff9240b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path where the CSV file is located\n",
    "folder_path = r\"S:\\microscopy\\Calcium imaging\\24.02.23-29 srx-9_FLP\\analysis_paper\\bout_analysis\"\n",
    "# Define the base name for the file\n",
    "file_name = 'intensity_and_speed_immobiliy_df_'\n",
    "# Define the strain name (e.g., \"3451\") to create a specific file name\n",
    "strain_name = '3451'\n",
    "# Construct the full file name by combining the base file name with the strain name\n",
    "strain_file_name =  file_name + strain_name + \".csv\"\n",
    "# Read the CSV file into a pandas DataFrame using the full file path\n",
    "data_df = pd.read_csv(os.path.join(folder_path, strain_file_name))\n",
    "# Display the contents of the DataFrame\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a40c2-600b-44b2-a64e-ec9260eea3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'replicate' and 'n_worm' to analyze each worm separately\n",
    "grouped_worms = worm_data_df.groupby(['replicate','n_worm'])\n",
    "# Sort each group by 'time' to ensure temporal order before further processing\n",
    "sorted_groups = grouped_worms.apply(lambda x: x.sort_values(by='time')).reset_index(drop=True)\n",
    "# Re-group the sorted data for further transformations\n",
    "regrouped_sorted = sorted_groups.groupby(['replicate', 'n_worm'])\n",
    "\n",
    "# Interpolate missing values in 'normalised_intensity' and 'speed' using linear interpolation\n",
    "worm_data_df['normalised_intensity'] = regrouped_sorted['normalised_intensity'].transform(lambda x: x.interpolate(method='linear', limit_direction='both'))\n",
    "worm_data_df['speed'] = regrouped_sorted['speed'].transform(lambda x: x.interpolate(method='linear', limit_direction='both'))\n",
    "\n",
    "# Re-group the data again after interpolation\n",
    "grouped_worms = worm_data_df.groupby(['replicate','n_worm'])\n",
    "\n",
    "# Apply Savitzky-Golay filtering for smoothing:\n",
    "# - 'smooth_intensity_hard': Intensity smoothed with a larger window (120 frames) for stronger smoothing\n",
    "# - 'smooth_intensity_soft': Intensity smoothed with a smaller window (20 frames) for less aggressive smoothing\n",
    "# - 'smooth_speed': Speed smoothed with a window of 20 frames\n",
    "worm_data_df['smooth_intensity_hard'] = grouped_worms['normalised_intensity'].transform(lambda x: scipy.signal.savgol_filter(x, 120, 0, mode='mirror'))\n",
    "worm_data_df['smooth_intensity_soft'] = grouped_worms['normalised_intensity'].transform(lambda x: scipy.signal.savgol_filter(x, 20, 0, mode='mirror'))\n",
    "worm_data_df['smooth_speed'] = grouped_worms['speed'].transform(lambda x: scipy.signal.savgol_filter(x, 20, 0, mode='mirror'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a729a-52c3-41ad-83c3-5a19871552f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect peaks in the 'smooth_intensity_hard' signal for each worm in the dataset\n",
    "peaks_list = grouped_worms.apply(\n",
    "    lambda group: scipy.signal.find_peaks(\n",
    "        group['smooth_intensity_hard'],  # Input signal to analyze\n",
    "        prominence=0.02,  # Minimum prominence (height difference from surrounding points) to detect peaks\n",
    "        width=0  # Minimum peak width (0 means no restriction)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the detected peaks for each worm group\n",
    "peaks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131adb24-ee36-4eff-9254-04399e8985ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns a new unique 'n_worm_new' ID for each worm within the same time point\n",
    "worm_data_df['n_worm_new'] = worm_data_df.groupby('time').cumcount() + 1\n",
    "\n",
    "# Create matrices (pivot tables) for different parameters over time, with worms as columns\n",
    "# The values are smoothed intensity, speed, and binary sleep state\n",
    "\n",
    "# Pivot table for smoothed intensity ('smooth_intensity_soft')\n",
    "intensity_matrix = worm_data_df.pivot(index='time', columns='n_worm_new', values='smooth_intensity_soft')\n",
    "# Pivot table for smoothed speed ('smooth_speed')\n",
    "speed_matrix = worm_data_df.pivot(index='time', columns='n_worm_new', values='smooth_speed')\n",
    "# Pivot table for sleep state ('binary_table')\n",
    "sleep_matrix = worm_data_df.pivot(index='time', columns='n_worm_new', values='binary_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb58d58-6b25-439c-8b02-c7edb99a3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries and lists for storing adjusted peak data\n",
    "adjusted_peak_dict = {}\n",
    "adjusted_peak_dict = {}\n",
    "adjusted_peak_dict_list = []\n",
    "\n",
    "# Get unique worm IDs\n",
    "worms_count = worm_data_df['n_worm_new'].unique().tolist()\n",
    "\n",
    "# Loop over each worm and its corresponding detected peaks\n",
    "for n_worm, peak_sublist in enumerate(peaks_list.to_list(), start=1):\n",
    "    adjusted_peak_list = []\n",
    "\n",
    "    # Iterate over detected peaks and their widths\n",
    "    for peak, width in zip(peak_sublist[0], peak_sublist[1]['widths']) :\n",
    "        left_limit = int(peak - width/2)-25\n",
    "        right_limit = int(peak + width/2)+25\n",
    "\n",
    "        # Ensure limits stay within bounds\n",
    "        if left_limit < 0:\n",
    "            left_limit = 0\n",
    "        if right_limit > len(intensity_matrix):\n",
    "            right_limit = len(intensity_matrix)\n",
    "        index_range = slice(left_limit, right_limit)\n",
    "        selected_sublist = intensity_matrix.iloc[:, (n_worm-1)][index_range]\n",
    "\n",
    "        # Find the index of the maximum intensity within the selected range (adjusted peak)\n",
    "        adjusted_peak_list.append(selected_sublist.idxmax())\n",
    "        \n",
    "    # Store adjusted peak information in a dictionary\n",
    "    adjusted_peak_dict = {\n",
    "        'n_worm_new': worms_count[n_worm - 1],  # Assign the corresponding worm ID\n",
    "        'peaks': list(sorted(set(adjusted_peak_list))),  # Adjusted peak locations\n",
    "        'peaks_old': list(sorted(set(peak_sublist[0])))  # Original peak locations\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    adjusted_peak_dict_list.append(adjusted_peak_dict)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "new_peaks_df = pd.DataFrame(adjusted_peak_dict_list)  \n",
    "new_peaks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec765efa-c660-4a82-a58b-95738b076c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the lists in 'peaks' and 'peaks_old' columns\n",
    "all_peaks = new_peaks_df['peaks'].explode().tolist() \n",
    "\n",
    "# Calculate the total count of all peaks\n",
    "total_peaks_count = len(all_peaks)\n",
    "\n",
    "# Display the total count\n",
    "print(\"Total Peaks Count:\", total_peaks_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa97f7-a34b-4245-b4c8-b4a4f7b2f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_peaks_df['num_peaks'] = new_peaks_df['peaks'].apply(len)\n",
    "\n",
    "# Specify the filename for saving the DataFrame to a CSV file\n",
    "peaks_raw = f'{strain_name}_peaks_x_position.csv'\n",
    "# Create the full path for saving the CSV file\n",
    "peaks_position_saving_path = os.path.join(folder_path, peaks_raw)\n",
    "# Save the DataFrame to a CSV file at the specified path\n",
    "new_peaks_df.to_csv(peaks_position_saving_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc2298-c9c8-4145-872b-e3d457d88629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'peaks' and 'peaks_old' columns from new_peaks_df to create peak_data_df\n",
    "peak_data_df = new_peaks_df.drop(['peaks', 'peaks_old'], axis=1)\n",
    "\n",
    "# Get the 'replicate' value for each worm by taking the first occurrence within each worm group\n",
    "replicate_per_worm = worm_data_df.groupby('n_worm_new')['replicate'].first()\n",
    "# Map the replicate values to the corresponding worms in peak_data_df\n",
    "peak_data_df['replicate'] = peak_data_df['n_worm_new'].map(replicate_per_worm)\n",
    "# Calculate the peak frequency per minute (assuming 5 minutes of observation time)\n",
    "peak_data_df['peaks_frequency'] = peak_data_df['num_peaks'] / 5\n",
    "\n",
    "# Assign strain name (commented out, can be enabled if needed)\n",
    "# peak_data_df['strain'] = strain_name\n",
    "\n",
    "# Specify the filename for saving the DataFrame to a CSV file\n",
    "peaks_to_save = f'{strain_name}_peak_data.csv'\n",
    "\n",
    "# Create the full path for saving the CSV file\n",
    "peaks_saving_path = os.path.join(folder_path, peaks_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file at the specified path\n",
    "peak_data_df.to_csv(peaks_saving_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85053b8d-d771-4330-a15e-7926822868d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define time window before and after each detected peak\n",
    "time_before_peak = 150\n",
    "time_after_peak = 350\n",
    "total_time = time_before_peak + time_after_peak\n",
    "\n",
    "# Initialize lists to store aligned matrices for peak intensity, speed, and sleep probability\n",
    "aligned_peaks_matrix_of_worms = []\n",
    "aligned_speed_matrix_of_worms = []\n",
    "aligned_sleep_matrix_of_worms = []\n",
    "\n",
    "# Retrieve the list of worm identifiers from 'new_peaks_df'\n",
    "worms = new_peaks_df['n_worm_new'].values.tolist()\n",
    "\n",
    "# Iterate over each worm in the dataset\n",
    "for worm in worms:\n",
    "    # Create empty matrices to store aligned peaks and speed data\n",
    "    aligned_peaks_matrix = np.empty((0, (total_time+1)))\n",
    "    aligned_speed_matrix = np.empty((0, (total_time+1)))\n",
    "    aligned_sleep_matrix = np.empty((0, (total_time+1)))\n",
    "    \n",
    "    # Filter peak data for the current worm\n",
    "    filtered_peaks = new_peaks_df.loc[new_peaks_df['n_worm_new'] == worm, 'peaks']\n",
    "    worm_df = worm_data_df[worm_data_df['n_worm_new'] == worm]  # Filter DataFrame for worm\n",
    "    \n",
    "    # Convert filtered peak data into a list\n",
    "    peaks_list = filtered_peaks.values.tolist()[0]\n",
    "    if len(peaks_list) == 0: # Skip worms that have no detected peaks\n",
    "        continue\n",
    "        \n",
    "    # Iterate over each detected peak for the current worm\n",
    "    for peak in peaks_list:\n",
    "        # Define the start and end indices for slicing data around the peak\n",
    "        x_start, x_end = peak - time_before_peak, peak + time_after_peak\n",
    "        \n",
    "        # Extract data for intensity, speed, and sleep probability from their respective matrices\n",
    "        original_bout = np.array([intensity_matrix.loc[x_start:x_end, worm].values])\n",
    "        original_speed = np.array([speed_matrix.loc[x_start:x_end, worm].values])\n",
    "        original_sleep = np.array([sleep_matrix.loc[x_start:x_end, worm].values])\n",
    "        \n",
    "        # Handle boundary cases where indices fall outside the available data range\n",
    "        if x_start < 0:\n",
    "             # Pad with NaNs if the extracted range starts before the first time point\n",
    "            bout = (np.append(np.repeat(np.nan, abs(x_start)), original_bout)).reshape(1,total_time+1)\n",
    "            speed = (np.append(np.repeat(np.nan, abs(x_start)), original_speed)).reshape(1,total_time+1)\n",
    "            sleep = (np.append(np.repeat(np.nan, abs(x_start)), original_sleep)).reshape(1,total_time+1)\n",
    "            aligned_peaks_matrix = np.append(aligned_peaks_matrix, bout, axis = 0)\n",
    "            aligned_speed_matrix = np.append(aligned_speed_matrix, speed, axis = 0)\n",
    "            aligned_sleep_matrix =  np.append(aligned_sleep_matrix, sleep, axis = 0)\n",
    "        \n",
    "        elif x_end > len(intensity_matrix)-1:\n",
    "            # Pad with NaNs if the extracted range exceeds the last available time point\n",
    "            bout = (np.append(original_bout, np.repeat(np.nan, (x_end - len(worm_df)+1)))).reshape(1,total_time+1)\n",
    "            speed = (np.append(original_speed, np.repeat(np.nan, (x_end - len(worm_df)+1)))).reshape(1,total_time+1)\n",
    "            sleep = (np.append(original_sleep, np.repeat(np.nan, (x_end - len(worm_df)+1)))).reshape(1,total_time+1)\n",
    "            aligned_peaks_matrix = np.append(aligned_peaks_matrix, bout, axis = 0)\n",
    "            aligned_speed_matrix = np.append(aligned_speed_matrix, speed, axis = 0)\n",
    "            aligned_sleep_matrix = np.append(aligned_sleep_matrix, sleep, axis = 0)\n",
    "        \n",
    "        else:\n",
    "            # Append the extracted data directly if within the available range\n",
    "            aligned_peaks_matrix = np.append(aligned_peaks_matrix, original_bout, axis = 0)\n",
    "            aligned_speed_matrix = np.append(aligned_speed_matrix, original_speed, axis = 0)\n",
    "            aligned_sleep_matrix = np.append(aligned_sleep_matrix, original_sleep, axis = 0)\n",
    "            \n",
    "    # Compute the mean-aligned matrices across all detected peaks for this worm\n",
    "    aligned_peaks_matrix_of_worms.append(np.nanmean(aligned_peaks_matrix, axis = 0))\n",
    "    aligned_speed_matrix_of_worms.append(np.nanmean(aligned_speed_matrix, axis = 0))\n",
    "    aligned_sleep_matrix_of_worms.append(np.nanmean(aligned_sleep_matrix, axis=0))\n",
    "                \n",
    "#np.savetxt(os.path.join(folder_path, strain_name+'_aligned_peaks_worm.csv'), aligned_peaks_matrix_of_worms, delimiter=\",\")\n",
    "\n",
    "#np.savetxt(os.path.join(folder_path, strain_name+'_aligned_speed_worm.csv'), aligned_speed_matrix_of_worms, delimiter=\",\")\n",
    "\n",
    "aligned_sleep_matrix_of_worms_to_save = [1-x for x in aligned_sleep_matrix_of_worms]\n",
    "#np.savetxt(os.path.join(folder_path, strain_name+'_aligned_sleep_worm.csv'), aligned_sleep_matrix_of_worms_to_save, delimiter=\",\")     \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e74dda-63c1-495c-984f-af764d61c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold each row of data\n",
    "data_rows = []\n",
    "\n",
    "# Iterate through each worm's data\n",
    "for worm_peak, worm_speed, worm_sleep, i in zip(aligned_peaks_matrix_of_worms, aligned_speed_matrix_of_worms, aligned_sleep_matrix_of_worms, worms_count):\n",
    "    \n",
    "    # Iterate through time points and store data\n",
    "    for j, (intensity, speed, sleep) in enumerate(zip(worm_peak, worm_speed, worm_sleep)):\n",
    "        data_rows.append({'n_worm_new': i, 'time': j, 'intensity': intensity, 'speed':speed, 'sleep':(1-sleep), 'strain': strain_name})\n",
    "\n",
    "# Create DataFrame\n",
    "worm_bout_df = pd.DataFrame(data_rows)\n",
    "\n",
    "# Specify the filename for saving the DataFrame to a CSV file\n",
    "worm_bout_to_save = f'{strain_name}_each_worm_peak_{time_after_peak}_min.csv'\n",
    "\n",
    "# Create the full path for saving the CSV file\n",
    "worm_bout_to_save_path = os.path.join(folder_path, worm_bout_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file at the specified path\n",
    "worm_bout_df.to_csv(worm_bout_to_save_path, index=False)\n",
    "\n",
    "worm_bout_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49390e49-eb65-45d9-a350-ad3342428020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Calculate the number of valid (non-NaN) entries for each time point across all worms\n",
    "valid_counts = np.sum(~np.isnan(aligned_sleep_matrix_of_worms), axis=0) \n",
    "\n",
    "# Calculate the mean of the aligned peaks across all worms (ignoring NaNs)\n",
    "aligned_peaks = np.nanmean(aligned_peaks_matrix_of_worms, axis = 0)\n",
    "# Calculate the standard deviation of aligned peaks and compute the error using the standard error of the mean\n",
    "error_peaks = np.nanstd(aligned_peaks_matrix_of_worms, axis=0)/np.sqrt(valid_counts)\n",
    "# Calculate the mean of the aligned speed values across all worms (ignoring NaNs)\n",
    "aligned_speed = np.nanmean(aligned_speed_matrix_of_worms, axis = 0)\n",
    "# Calculate the standard deviation of aligned speed values and compute the error using the standard error of the mean\n",
    "error_speed = np.nanstd(aligned_speed_matrix_of_worms, axis=0)/np.sqrt(valid_counts)\n",
    "\n",
    "# Calculate the sleep probability by computing the average across all worms (ignoring NaNs) and subtracting from 1\n",
    "aligned_sleep = 1 - np.nansum(aligned_sleep_matrix_of_worms, axis=0)/ valid_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e7ad3-59fc-4bbd-8523-9ea5890a08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data into a dictionary\n",
    "data = {\n",
    "    'Mean Vector Speed': aligned_speed,  # Mean speed for each time point\n",
    "    'Error Speed': error_speed,  # Standard error for the speed values\n",
    "    'Mean Vector Intensity': aligned_peaks,  # Mean intensity for each time point\n",
    "    'Error Intensity': error_peaks,  # Standard error for the intensity values\n",
    "    'Sleep probability': aligned_sleep  # Sleep probability (1 - wakefulness)\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame from the data dictionary\n",
    "saving_df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the filename for saving the DataFrame to a CSV file\n",
    "file_to_save = f'{strain_name}_peak_alignment_{time_after_peak}min.csv'\n",
    "\n",
    "# Create the full path for saving the CSV file\n",
    "saving_path = os.path.join(folder_path, file_to_save)\n",
    "\n",
    "# Save the DataFrame to a CSV file at the specified path\n",
    "saving_df.to_csv(saving_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
